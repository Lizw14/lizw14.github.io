[{"authors":["admin"],"categories":null,"content":"I am a 5th-year PhD student at Johns Hopkins University, supervised by Prof. Alan Yuille and Prof. Benjamin Van Durme. I am a member of CCVL lab. Before this, I received my B.E. degree at Tsinghua Univeristy in 2018. I have also spent wonderful times at Facebook, Adobe and Sensetime.\nMy research interest focuses on computer vision and natural language processing. My works relates to both large-scale pretraining and compositional models. I am also interested in model diagnosis including robustness, generaliation, compositionality, etc. I believe that the joint learning of vision and language offers mutual benefits!\nI am on the job market  for research positions in 2023, including full-time/internship positions in industry and post-docs positions. Don\u0026rsquo;t hesitate to reach out to me (at zli110 at jhu dot edu) if there is a potential fit:)\nIn part time, I am a big fan of outdoor sports including rock climbing (recovering from a finger injury ðŸ˜­), snowboarding, skiing, hiking, mountaineering, etc. Recently I also spent time in learning piano.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://lizw14.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I am a 5th-year PhD student at Johns Hopkins University, supervised by Prof. Alan Yuille and Prof. Benjamin Van Durme. I am a member of CCVL lab. Before this, I received my B.","tags":null,"title":"Zhuowan Li","type":"authors"},{"authors":["**Zhuowan Li**","Cihang Xie","Benjamin Van Durme","Alan Yuille"],"categories":[],"content":"","date":1670025600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1670025600,"objectID":"1e392079c2ed28da12a104c5b6081beb","permalink":"https://lizw14.github.io/publication/visual_probing/","publishdate":"2022-12-03T18:11:40-04:00","relpermalink":"/publication/visual_probing/","section":"publication","summary":"","tags":[],"title":"Localization vs. Semantics: How Can Language Benefit Visual Representation Learning?","type":"publication"},{"authors":["**Zhuowan Li**","Xingrui Wang","Elias Stengel-Eskin","Adam Kortylewski","Wufei Ma","Benjamin Van Durme","Alan Yuille"],"categories":[],"content":"","date":1669766400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1669766400,"objectID":"fd75ff0e9abd131bda6f9c54997c3056","permalink":"https://lizw14.github.io/publication/superclevr/","publishdate":"2022-11-30T18:11:40-04:00","relpermalink":"/publication/superclevr/","section":"publication","summary":"","tags":[],"title":"Super-CLEVR: A Virtual Benchmark to Diagnose Domain Robustness in Visual Reasoning","type":"publication"},{"authors":["Chenyu Zhang","Benjamin Van Durme","**Zhuowan Li***","Elias Stengel-Eskin*"],"categories":[],"content":"","date":1653868800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1653868800,"objectID":"98ef6260ac678176b696e6e0f73831b5","permalink":"https://lizw14.github.io/publication/22_commonsense/","publishdate":"2022-11-30T18:11:40-04:00","relpermalink":"/publication/22_commonsense/","section":"publication","summary":"","tags":[],"title":"Visual Commonsense in Pretrained Unimodal and Multimodal Models","type":"publication"},{"authors":["Vipul Gupta","**Zhuowan Li**","Adam Kortylewski","Chenyu Zhang","Yingwei Li","Alan Yuille"],"categories":[],"content":"","date":1651276800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1651276800,"objectID":"82f0922c8b9722c51986470a857f1bab","permalink":"https://lizw14.github.io/publication/swapmix/","publishdate":"2022-11-30T18:11:40-04:00","relpermalink":"/publication/swapmix/","section":"publication","summary":"","tags":[],"title":"SwapMix: Diagnosing and Regularizing the Over-Reliance on Visual Context in Visual Question Answering","type":"publication"},{"authors":["**Zhuowan Li**","Elias Stengel-Eskin","Yixiao Zhang","Cihang Xie","Quan Tran","Benjamin Van Durme","Alan Yuille"],"categories":[],"content":"","date":1633642000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1633642000,"objectID":"7a65606448f4045eed201e4240abd956","permalink":"https://lizw14.github.io/project/ccosr/","publishdate":"2021-10-07T17:26:40-04:00","relpermalink":"/project/ccosr/","section":"project","summary":"","tags":[],"title":"Calibrating Concepts and Operations: Towards Symbolic Reasoning  on  Real Images","type":"project"},{"authors":["**Zhuowan Li**","Elias Stengel-Eskin","Yixiao Zhang","Cihang Xie","Quan Tran","Benjamin Van Durme","Alan Yuille"],"categories":[],"content":"","date":1632960000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1632960000,"objectID":"b4d3d0345fefcdf5fd690381f5468a77","permalink":"https://lizw14.github.io/publication/ccosr/","publishdate":"2021-09-30T18:11:40-04:00","relpermalink":"/publication/ccosr/","section":"publication","summary":"","tags":[],"title":"Calibrating Concepts and Operations: Towards Symbolic Reasoning  on  Real Images","type":"publication"},{"authors":["**Zhuowan Li**","Quan Tran","Long Mai","Zhe Lin","Alan Yuille"],"categories":[],"content":"   Context-ware group captioning. Given a group of target images (shown in orange boxes) and a group of reference images which provide the context (woman), the goal is to generatea language description (woman with cowboy hat) that best describes the target group while taking into account the context depicted by the reference group.   Abstract While image captioning has progressed rapidly, existing works focus mainly on describing single images. In this paper, we introduce a new task, context-aware group captioning, which aims to describe a group of target images in the context of another group of related reference images. Context-aware group captioning requires not only summarizing information from both the target and reference image group but also contrasting between them. To solve this problem, we propose a framework combining self-attention mechanism with contrastive feature construction to effectively summarize common information from each image group while capturing discriminative information between them. To build the dataset for this task, we propose to group the images and generate the group captions based on single image captions using scene graphs matching. Our datasets are constructed on top of the public Conceptual Captions dataset and our new Stock Captions dataset. Experiments on the two datasets show the effectiveness of our method on this new task.\nDatasets   Table: Statistics of Conceptual Captions and Stock Captions    Conceptual Captions Dataset   Download.   Stock Captions Dataset  Coming up soon.    Code Code in Pytorch coming up soon\n","date":1585002154,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585002154,"objectID":"365c79d1d362fe57d60eab1b8330ef77","permalink":"https://lizw14.github.io/project/groupcap/","publishdate":"2020-03-23T18:22:34-04:00","relpermalink":"/project/groupcap/","section":"project","summary":"Context-ware group captioning. Given a group of target images (shown in orange boxes) and a group of reference images which provide the context (woman), the goal is to generatea language description (woman with cowboy hat) that best describes the target group while taking into account the context depicted by the reference group.","tags":[],"title":"Context-Aware Group Captioning via Self-Attention and Contrastive Features","type":"project"},{"authors":["**Zhuowan Li**","Quan Tran","Long Mai","Zhe Lin","Alan Yuille"],"categories":[],"content":"","date":1581379200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1581379200,"objectID":"35c42f0d762196af539b5cf3bff01c85","permalink":"https://lizw14.github.io/publication/groupcap/","publishdate":"2020-02-11T00:00:00Z","relpermalink":"/publication/groupcap/","section":"publication","summary":"","tags":[],"title":"Context-Aware Group Captioning via Self-Attention and Contrastive Features","type":"publication"},{"authors":["Yixiao Ge*","**Zhuowan Li***","Haiyu Zhao","Guojun Yin","Shuai Yi","Xiaogang Wang","and Hongsheng Li"],"categories":[],"content":"","date":1544227200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1544227200,"objectID":"c8005a3944d82ef9d67b5b743c209c7a","permalink":"https://lizw14.github.io/publication/fdgan/","publishdate":"2018-12-08T00:00:00Z","relpermalink":"/publication/fdgan/","section":"publication","summary":"","tags":[],"title":"FD-GAN: Pose-guided Feature Distilling GAN for Robust Person Re-identification","type":"publication"}]