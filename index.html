<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Zhuowan Li</title>
  
  <meta name="author" content="Zhuowan Li">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <p style="text-align:center">
              <name>Zhuowan Li (ÊùéÂçìÂ©â)</name>
            </p>
            <hr>
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p>
                I am a software engineer at Google Deepmind. I am currently working on Gemini post-training, with a focus on personalization and open-ended questions. 
              </p>
              <p>
                I finished my Ph.D. in Feb 2024 from <a href="https://www.jhu.edu">Johns Hopkins University</a>, co-advised by Prof. <a href="https://cs.jhu.edu/~ayuille/">Alan Yuille</a> and <a href="https://www.cs.jhu.edu/~vandurme/index.html">Benjamin Van Durme</a>. I am a member of the <a href="https://ccvl.jhu.edu/team/">CCVL lab</a>. I received my B.E. degree from <a href="https://www.tsinghua.edu.cn/en/">Tsinghua Univeristy</a> in 2018, where I double major in Electronic Engineering and Journalism and Communication. I have also interned at <a href="https://aws.amazon.com">Amazon AWS</a>, <a href="https://ai.facebook.com">Meta AI</a>, <a href="https://research.adobe.com/">Adobe Research</a> and <a href="https://www.sensetime.com/en">Sensetime</a>.
              </p>
              <!-- <p>
                My research interest focuses on computer vision and natural language processing. My works relates to both large-scale pretraining and compositional models. I am also interested in model diagnosis including robustness, generaliation, compositionality, etc. I believe that the joint learning of vision and language offers mutual benefits.
              </p> -->
              <p>
                In part time, I am a big fan of outdoor sports including rock climbing, snowboarding, skiing, hiking, mountaineering, etc. I am learning tennis recently.
              </p>
              <p style="text-align:center">
                <!-- <a href="mailto:zli110@jhu.edu">Email</a> &nbsp/&nbsp -->
                <a href="bio/CV_Zhuowan_Li.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=Fft1WvwAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://twitter.com/zhuowanli">Twitter</a> &nbsp/&nbsp
                <a href="https://github.com/Lizw14">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:30%;max-width:30%">
              <a href="bio/avatar.jpg"><img style="width:100%;max-width:100%;border-radius:50%" alt="profile photo" src="bio/avatar.jpg" class="hoverZoomLink"></a>
              <p style="text-align:center"> <a href="mailto:lizhuowan14@gmail.com">Email</a>: lizhuowan14 at gmail dot com</p>
            </td>
            
          </tr>
        </tbody></table>

        <!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> -->
          <!-- <tr> -->
              <!-- <td style="padding:10px;width:100%;vertical-align:middle"> -->
                  <!-- <p> -->
                    <!-- <span style="background-color: #FFFF00"> -->
                    <!-- <font color="orange"><strong>On the job market</strong></font>: I am currently on the job market. I am interested in both industry and academic positions. Don‚Äôt hesitate to email me if there is a potential fit.</p> -->
              <!-- </td> -->
          <!-- </tr> -->
      <!-- </tbody></table> -->
      <hr>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>News</heading>
              <!-- <p>
                  <li><span style="background-color: #FFFF00"> New </span><font color="orange"><strong>On the job market</strong></font>: I am currently on the job market. I am interested in both industry and academic positions. Don‚Äôt hesitate to email me if there is a potential fit.</p>
              <ul> -->
                <!-- <li><font color="orange"><strong>[June 2024]</strong> I will attend CVPR 2024 in person at Seattle. Happy to chat!</font></li> -->
                <li><strong>[Nov 2024]</strong> I will attend EMNLP 2024 in person at Miami. Happy to connect!</li>
                <li><strong>[June 2024]</strong> I will attend CVPR 2024 in person at Seattle. Happy to chat!</li>
                <li><strong>[Feb 2024]</strong> I graduated from JHU and joined Google as a software engineer!</li>
                <li><strong>[June 2023]</strong> I will attend CVPR 2023 in person at Vancouver. Let me know if you want to talk with me!</li>
                <li><strong>[May 2023]</strong> Started as as applied scentist intern at Amazon AWS.</li>
                <li><strong>[May 2023]</strong> Invited talk at <a href="https://cocosci.mit.edu/josh"> the Computational Cognitive Science Lab </a> at <a href="https://www.mit.edu">MIT</a>.</li>
                <li><strong>[February 2023]</strong> Super-CLEVR is accepted by CVPR 2023 as Highlight.</li>
              </ul>
                <p style="text-align:right;font-size:small;">
                  Last updated: 2025/08/12.
                </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        
        <hr>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Publications</heading>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

    <!-- <tr bgcolor="#ffffd0"> -->
    <tr>
      <td style="padding:20px;width:35%;vertical-align:middle">
          <img src='images/2025_gemini_25.png' width="250">
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle">
        <a href="https://arxiv.org/abs/2507.06261.pdf">
          <papertitle>Gemini 2.5: Pushing the Frontier with Advanced Reasoning, Multimodality, Long Context, and Next Generation Agentic Capabilities</papertitle>
        </a>
        <br>
        <strong>Gemini Team, Google</strong>
        <br>
        <em>Technical Report</em>, 2025
        <br>       
        <a href="https://arxiv.org/abs/2507.06261.pdf">arXiv</a>
        <p></p>
      </td>
    </tr>

    <tr>
      <td style="padding:20px;width:35%;vertical-align:middle">
          <img src='images/2025_chart.png' width="250">
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle">
        <a href="https://arxiv.org/pdf/2508.06492.pdf">
          <papertitle>Effective Training Data Synthesis for Improving MLLM Chart Understanding</papertitle>
        </a>
        <br>
        Yuwei Yang, Zeyu Zhang, Yunzhong Hou, <strong>Zhuowan Li</strong>, Gaowen Liu, Ali Payani, Yuan-Sen Ting, 
        <a href="https://zheng-lab-anu.github.io">Liang Zheng</a>
        <br>
        <em>ICCV</em>, 2025
        <br>       
        <a href="https://arxiv.org/pdf/2508.06492.pdf">arXiv</a>
        <p></p>
      </td>
    </tr>

    <tr>
      <td style="padding:20px;width:35%;vertical-align:middle">
          <img src='images/2024_rag_lc.png' width="250">
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle">
        <a href="https://arxiv.org/pdf/2407.16833.pdf">
          <papertitle>Retrieval Augmented Generation or Long-Context LLMs? A Comprehensive Study and Hybrid Approach</papertitle>
        </a>
        <br>
        <strong>Zhuowan Li</strong>,
        <a href="https://scholar.google.com/citations?user=kMRdKRUAAAAJ&hl=en">Cheng Li</a>,
        <a href="https://scholar.google.com/citations?user=GJ5RQz4AAAAJ&hl=en">Mingyang Zhang</a>,
        <a href="https://websites.umich.edu/~qmei/">Qiaozhu Mei</a>,
        <a href="https://bendersky.github.io/">Michael Bendersky</a>
        <br>
        <em>EMNLP Industry Track</em>, 2024
        <br>       
        <a href="https://arxiv.org/pdf/2407.16833.pdf">arXiv</a> /
        <a href="project/2024_RAG_LC/poster.pdf">poster</a>
        <p></p>
      </td>
    </tr>

    <tr>
      <td style="padding:20px;width:35%;vertical-align:middle">
          <img src='images/2024_exovip.png' width="250">
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle">
        <a href="https://arxiv.org/pdf/2408.02210.pdf">
          <papertitle>ExoViP: Step-by-step Verification and Exploration with Exoskeleton Modules for Compositional Visual Reasoning</papertitle>
        </a>
        <br>
        <a href="https://patrick-tssn.github.io/">Yuxuan Wang</a>,
        <a href="https://cs.jhu.edu/~ayuille/">Alan Yuille</a>,
        <strong>Zhuowan Li</strong>*,
        <a href="https://zilongzheng.github.io/">Zilong Zheng</a>*
        <br>
        <em>COLM</em>, 2024
        <br>       
        <a href="https://arxiv.org/pdf/2408.02210.pdf">arXiv</a> /
        <a href="https://github.com/bigai-nlco/ExoViP">code</a>
        <p></p>
      </td>
    </tr>

    <tr>
      <td style="padding:20px;width:35%;vertical-align:middle">
          <img src='images/2024_lamenda.png' width="250">
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle">
        <a href="https://arxiv.org/pdf/2403.16385.pdf">
          <papertitle>Synthesize Step-by-Step: Tools, Templates and LLMs as Data Generators for Reasoning-Based Chart VQA</papertitle>
        </a>
        <br>
        <strong>Zhuowan Li</strong>*,
        <a href="https://bhavanj.github.io">Bhavan Jasani</a>*,
        <a href="https://scholar.google.co.jp/citations?user=h_oYR-IAAAAJ&hl=en">Peng Tang</a>,
        <a href="https://www.linkedin.com/in/shghadar/">Shabnam Ghadar</a>
        <br>
        <em>CVPR</em>, 2024
        <br>       
        <a href="https://arxiv.org/pdf/2403.16385.pdf">arXiv</a>
        <p></p>
      </td>
    </tr>

    <tr>
      <td style="padding:20px;width:35%;vertical-align:middle">
          <img src='images/2024_causal_cog.png' width="250">
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle">
        <a href="https://arxiv.org/pdf/2312.06685.pdf">
          <papertitle>Causal-CoG: A Causal-Effect Look at Context Generation for Boosting Multi-modal Language Models</papertitle>
        </a>
        <br>
        <a href="https://zhaoshitian.github.io">Shitian Zhao</a>,
        <strong>Zhuowan Li</strong>,
        Yadong Lu,
        <a href="https://cs.jhu.edu/~ayuille/">Alan Yuille</a>,
        <a href="https://wangyan921.github.io">Yan Wang</a>
        <br>
        <em>CVPR (Highlight, top 2.8%)</em>, 2024
        <br>       
        <a href="https://arxiv.org/pdf/2312.06685.pdf">arXiv</a> /
        <a href="https://github.com/zhaoshitian/Causal-CoG">code</a>
        <p></p>
      </td>
    </tr>

    <tr>
      <td style="padding:20px;width:35%;vertical-align:middle">
          <img src='images/2024_thesis.jpg' width="250">
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle">
        <a href="https://jscholarship.library.jhu.edu/items/61b048ea-2502-47fa-a2de-40d88e666b4a">
          <papertitle>On the Diagnosis and Generalization of Compositional Visual Reasoning</papertitle>
        </a>
        <br>
        <strong>Zhuowan Li</strong>
        <br>
        <em>Ph.D. thesis</em>, 2024
        <br>
        <a href="bio/PHD_thesis_JHU.pdf">pdf</a> 
        <p></p>
      </td>
    </tr>

    <tr>
      <td style="padding:20px;width:35%;vertical-align:middle">
          <img src='images/2022_visual_probing.png' width="250">
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle">
        <a href="https://arxiv.org/pdf/2212.00281.pdf">
          <papertitle>Localization vs. Semantics: How Can Language Benefit Visual Representation Learning?</papertitle>
        </a>
        <br>
        <strong>Zhuowan Li</strong>,
        <a href="https://cihangxie.github.io/">Cihang Xie</a>,
        <a href="https://www.cs.jhu.edu/~vandurme/index.html">Benjamin Van Durme</a>,
        <a href="https://cs.jhu.edu/~ayuille/">Alan Yuille</a>
        <br>
        <em>EACL</em>, 2024
        <br>
        <a href="https://arxiv.org/pdf/2212.00281.pdf">arXiv</a> /
        <a href="https://github.com/Lizw14/visual_probing">code</a> (to be released)
        <p></p>
      </td>
    </tr>

    <tr>
      <td style="padding:20px;width:35%;vertical-align:middle">
          <img src='images/2023_3D_superclevr.png' width="250">
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle">
        <a href="https://arxiv.org/pdf/2310.17914.pdf">
          <papertitle>3D-Aware Visual Question Answering
            about Parts, Poses and Occlusions</papertitle>
        </a>
        <br>
        <a href="https://xingruiwang.github.io">Xingrui Wang</a>,
        <a href="https://wufeim.github.io">Wufei Ma</a>,
        <strong>Zhuowan Li</strong>,
        <a href="https://adamkortylewski.com">Adam Kortylewski</a>,
        <a href="https://cs.jhu.edu/~ayuille/">Alan Yuille</a>
        <br>
        <em>NeurIPS</em>, 2023
        <br>       
        <a href="https://arxiv.org/pdf/2310.17914.pdf">arXiv</a> /
        <a href="https://github.com/XingruiWang/3D-Aware-VQA">code and dataset</a>
        <p></p>
      </td>
    </tr>

    <tr>
      <td style="padding:20px;width:35%;vertical-align:middle">
          <img src='images/2023_superclevr.png' width="250">
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle">
        <a href="https://lizw14.github.io/project/2023_SuperCLEVR">
          <papertitle>Super-CLEVR: A Virtual Benchmark to Diagnose Domain Robustness in Visual Reasoning</papertitle>
        </a>
        <br>
        <strong>Zhuowan Li</strong>,
        <a href="https://xingruiwang.github.io">Xingrui Wang</a>,
        <a href="https://esteng.github.io/">Elias Stengel-Eskin</a>,
        <a href="https://adamkortylewski.com">Adam Kortylewski</a>,
        <a href="https://wufeim.github.io">Wufei Ma</a>,
        <a href="https://www.cs.jhu.edu/~vandurme/index.html">Benjamin Van Durme</a>,
        <a href="https://cs.jhu.edu/~ayuille/">Alan Yuille</a>
        <br>
        <em>CVPR (Highlight, top 2.5%)</em>, 2023
        <br>
        <a href="https://lizw14.github.io/project/2023_SuperCLEVR">project page</a> /
        <a href="https://arxiv.org/pdf/2212.00259.pdf">arXiv</a> /
        <a href="https://github.com/Lizw14/Super-CLEVR">code and dataset</a>
        <p></p>
      </td>
    </tr>


    <tr>
      <td style="padding:20px;width:35%;vertical-align:middle">
          <img src='images/2022_vl_commonsense.png' width="250">
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle">
        <a href="https://arxiv.org/pdf/2205.01850.pdf">
          <papertitle>Visual Commonsense in Pretrained Unimodal and Multimodal Models</papertitle>
        </a>
        <br>
        <a href="https://chenyu-zhang.appspot.com">Chenyu Zhang</a>
        <a href="https://www.cs.jhu.edu/~vandurme/index.html">Benjamin Van Durme</a>,
        <strong>Zhuowan Li*</strong>,
        <a href="https://esteng.github.io/">Elias Stengel-Eskin*</a>,
        <br>
        <em>NAACL (Oral)</em>, 2022
        <br>
        <a href="https://arxiv.org/pdf/2205.01850.pdf">arXiv</a> /
        <a href="https://github. com/ChenyuHeidiZhang/VL-commonsense">code and dataset</a>
        <p></p>
      </td>
    </tr>

    <tr>
      <td style="padding:20px;width:35%;vertical-align:middle">
          <img src='images/2022_swapmix.png' width="250">
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle">
        <a href="https://arxiv.org/pdf/2204.02285.pdf">
          <papertitle>SwapMix: Diagnosing and Regularizing the Over-Reliance on Visual Context in Visual Question Answering</papertitle>
        </a>
        <br>
        <a href="https://vipulgupta1011.github.io">Vipul Gupta</a>,
        <strong>Zhuowan Li</strong>,
        <a href="https://adamkortylewski.com">Adam Kortylewski</a>,
        <a href="https://angelicaz.github.io">Chenyu Zhang</a>,
        <a href="https://yingwei.li">Yingwei Li</a>,
        <a href="https://cs.jhu.edu/~ayuille/">Alan Yuille</a>
        <br>
        <em>CVPR</em>, 2022
        <br>
        <a href="https://arxiv.org/pdf/2204.02285.pdf">arXiv</a> /
        <a href="https://github.com/vipulgupta1011/swapmix">code</a>
        <p></p>
      </td>
    </tr>

    <tr>
      <td style="padding:20px;width:35%;vertical-align:middle">
          <img src='images/2021_calico.png' width="250">
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle">
        <a href="https://arxiv.org/pdf/2110.00519.pdf">
          <papertitle>Calibrating Concepts and Operations: Towards Symbolic Reasoning on Real Images</papertitle>
        </a>
        <br>
        <strong>Zhuowan Li</strong>,
        <a href="https://esteng.github.io/">Elias Stengel-Eskin</a>,
        <a href="https://scholar.google.com/citations?user=lU3wroMAAAAJ&hl=zh-CN">Yixiao Zhang</a>,
        <a href="https://cihangxie.github.io/">Cihang Xie</a>,
        <a href="https://scholar.google.com/citations?user=ehs5ImcAAAAJ&hl=en">Quan Tran</a>,
        <a href="https://www.cs.jhu.edu/~vandurme/index.html">Benjamin Van Durme</a>,
        <a href="https://cs.jhu.edu/~ayuille/">Alan Yuille</a>
        <br>
        <em>ICCV</em>, 2021
        <br>
        <a href="https://arxiv.org/pdf/2110.00519.pdf">arXiv</a> /
        <a href="https://github.com/Lizw14/CaliCO.git">code</a>
        <p></p>
      </td>
    </tr>

    

    <tr>
      <td style="padding:20px;width:35%;vertical-align:middle">
          <img src='images/2020_groupcap.png' width="250">
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle">
        <a href="https://lizw14.github.io/project/groupcap">
          <papertitle>Context-Aware Group Captioning via Self-Attention and Contrastive Features</papertitle>
        </a>
        <br>
        <strong>Zhuowan Li</strong>,
        <a href="https://scholar.google.com/citations?user=ehs5ImcAAAAJ&hl=en">Quan Tran</a>,
        <a href="https://mai-t-long.com">Long Mai</a>,
        <a href="https://scholar.google.com/citations?user=R0bnqaAAAAAJ&hl=en">Zhe Lin</a>,
        <a href="https://cs.jhu.edu/~ayuille/">Alan Yuille</a>
        <br>
        <em>CVPR</em>, 2020
        <br>
        <a href="https://arxiv.org/pdf/2004.03708.pdf">arXiv</a> /
        <a href="https://lizw14.github.io/project/groupcap">project page</a>
        <p></p>
      </td>
    </tr>

    <tr>
      <td style="padding:20px;width:35%;vertical-align:middle">
          <img src='images/2018_fdgan.png' width="250">
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle">
        <a href="https://arxiv.org/pdf/1810.02936.pdf">
          <papertitle>FD-GAN: Pose-guided Feature Distilling GAN for Robust Person Re-identification</papertitle>
        </a>
        <br>
        <a href="https://geyixiao.com">Yixiao Ge*</a>,
        <strong>Zhuowan Li*</strong>,
        <a href="https://scholar.google.com/citations?user=oGM5N1kAAAAJ">Haiyu Zhao</a>,
        <a href="https://gjyin91.github.io/">Guojun Yin</a>,
        <a href="https://scholar.google.com/citations?user=afbbNmwAAAAJ">Shuai Yi</a>,
        <a href="https://www.ee.cuhk.edu.hk/~xgwang/">Xiaogang Wang</a>,
        <a href="http://www.ee.cuhk.edu.hk/~hsli/">Hongsheng Li</a>
        <br>
        <em>NeurIPS</em>, 2018
        <br>
        <a href="https://arxiv.org/pdf/1810.02936.pdf">arXiv</a> /
        <a href="https://geyixiao.com/projects/fdgan.html">project page</a> /
        <a href="https://github.com/yxgeee/FD-GAN">code</a>
        <p></p>
      </td>
    </tr>
					
					
    </tbody></table>
    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr>
        <td style="padding:0px">
          <br>
          <p style="text-align:right;font-size:small;">
            Website theme stolen from <a href="https://jonbarron.info/">Jon Barron</a>.
          </p>
        </td>
      </tr>
    </tbody></table>

      </td>
    </tr>
  </table>
</body>

</html>
